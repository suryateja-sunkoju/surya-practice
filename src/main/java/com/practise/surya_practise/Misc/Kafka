https://www.youtube.com/watch?v=c7LPlWvxZcQ

https://www.geeksforgeeks.org/apache-kafka/kafka-interview-questions/
1. What is Apache Kafka?
Apache Kafka is a distributed streaming platform for publishing, subscribing, storing, and processing streams of records in real-time. It's designed to handle high-throughput, fault-tolerant, and scalable data pipelines. Kafka is often used for building real-time data pipelines and streaming applications.

2. What are the key components of Kafka?
Producer: Publishes messages to Kafka topics.
Consumer: Subscribes to topics and processes the published messages.
Broker: A Kafka server that stores and manages topics.
ZooKeeper: Manages and coordinates Kafka brokers.
Topic: A category or feed name to which records are published.
Partition: Topics are divided into partitions for scalability.


3. What is a topic in Kafka?
A topic in Kafka is a feed name to which records are published. Topics are multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it. Topics are split into partitions for improved scalability and parallel processing.

4. What is a partition in Kafka?

A partition is an ordered, immutable sequence of records that is continually appended to. Each partition is a structured commit log, and records in the partitions are each assigned a sequential id number called the offset. Partitions allow Kafka to scale horizontally and provide parallel processing capabilities.

5. What is the role of ZooKeeper in Kafka?
ZooKeeper is used for managing and coordinating Kafka brokers. It serves as a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.  ZooKeeper keeps track of the status of Kafka cluster nodes, Kafka topics, and partitions.

6. What is a broker in Kafka?
A broker is a Kafka server that runs in a Kafka cluster. It receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been published.

7. How does Kafka ensure fault tolerance?
Kafka ensures fault tolerance through data replication. Each partition is replicated across a configurable number of servers for fault tolerance. One of the servers is designated as the leader, which handles all read and write requests for the partition, while the others are followers that passively replicate the leader.

8. What is the difference between a Kafka consumer and consumer group?
A Kafka consumer is an application that reads data from Kafka topics. A consumer group is a set of consumers that work together to consume data from one or more topics. The key difference is that each message is delivered to one consumer instance within each subscribing consumer group. This allows for parallel processing and load balancing of topic consumption.

9. What is the purpose of the offset in Kafka?
The offset is a unique identifier of a record within a partition. It denotes the position of the consumer in the partition. Kafka maintains this offset per partition, per consumer group, allowing each consumer group to read from a different position in the partition. This enables Kafka to provide both queue and publish-subscribe messaging models.

10. How does Kafka handle message delivery semantics?
Kafka supports three message delivery semantics:
At most once: Messages may be lost but are never redelivered.
At least once: Messages are never lost but may be redelivered.
Exactly once: Each message is delivered once and only once. The choice depends on the specific use case and can be configured through producer and consumer settings.

11. What is the role of the Kafka producer API?
The Kafka producer API is used to publish streams of records to Kafka topics. It handles partitioning of messages, compression, and load balancing across multiple brokers. The producer is also responsible for retrying failed publish attempts and can be configured for different levels of delivery guarantees.

12. How does Kafka support scalability?
Kafka supports scalability through partitioning and distributed processing. Topics can be partitioned across multiple brokers, allowing for parallel processing. Consumers can be grouped to read from multiple partitions simultaneously. Brokers can be added to a cluster to increase capacity, and the cluster can be scaled without downtime.

13. What is log compaction in Kafka?
Log compaction is a mechanism to give finer-grained per-record retention, rather than the coarser-grained time-based retention. The idea is to selectively remove records where we have a more recent update with the same primary key. This way, the log is guaranteed to have at least the last state for each key.

14. How does Kafka handle message ordering?
Kafka guarantees order within a partition. Messages sent by a producer to a particular topic partition will be appended in the order they are sent. A consumer instance will read records in the order they are stored in the log. However, there's no guarantee of order across partitions.

15. What is the significance of the acks parameter in Kafka producers?
The acks parameter in Kafka producers controls the number of acknowledgments the producer requires the leader to have received before considering a request complete. It affects the durability of records and can be set to: 0: No acknowledgment 1: Leader acknowledgment only all: Full ISR (In-Sync Replica) acknowledgment

16. How does Kafka handle data retention?
Kafka handles data retention through configurable retention policies. These can be based on time (e.g., retain data for 7 days) or size (e.g., retain up to 1GB per partition). After the retention limit is reached, old messages are discarded. Kafka also supports log compaction for topics where only the latest value for each key is needed.

17. What is the purpose of the Kafka Connect API?
Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other data systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. This can be used to connect Kafka with databases, key-value stores, search indexes, and file systems.

18. How does Kafka ensure high availability?
Kafka ensures high availability through:
Replication of partitions across multiple brokers
Automatic leader election when a broker fails
Ability to add brokers to a cluster without downtime
Configurable number of in-sync replicas for durability
ZooKeeper for distributed coordination and broker management


19. What is the difference between Kafka Streams and Apache Flink?
While both Kafka Streams and Apache Flink are stream processing frameworks, they have some key differences:
Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. Flink is a distributed processing engine that can work with various data sources and sinks.
Kafka Streams is tightly integrated with Kafka, while Flink has a more general-purpose design.
Flink generally offers lower latency and higher throughput for complex operations, while Kafka Streams is simpler to deploy and operate.


20. How does Kafka handle message compression?
Kafka supports message compression to reduce the size of data transferred and stored. Compression can be configured at the producer level, and Kafka supports several compression types including gzip, snappy, lz4, and zstd. The broker can be configured to decompress messages to validate and convert them to the message format version on the broker.

21. What is the purpose of the Kafka Streams API?
The Kafka Streams API is a client library for building applications and microservices that process and analyze data stored in Kafka. It enables you to build stream processing applications with just standard Java and Kafka clients, without the need for a separate processing cluster. It supports stateful operations, windowing, joining streams and tables, and more.

22. How does Kafka handle message size limits?
Kafka has configurable message size limits. The default maximum message size is 1MB, but this can be increased by changing the 'message.max.bytes' configuration on the broker and the 'max.request.size' on the producer. However, very large messages can impact performance and memory usage, so it's generally recommended to keep messages relatively small.

23. What is the role of the group coordinator in Kafka?
The group coordinator in Kafka is responsible for managing consumer groups. It handles consumer group membership, assigns partitions to consumers within a group, and manages offset commits. When a consumer joins or leaves a group, the group coordinator triggers a rebalance to reassign partitions among the remaining consumers.

24. How does Kafka handle data replication?
Kafka replicates data by maintaining multiple copies of each partition across different brokers. One broker is designated as the leader for a partition, handling all read and write requests, while others are followers that replicate the leader's data. If a leader fails, one of the followers becomes the new leader. The number of replicas is configurable per topic.

25. What is the purpose of the Idempotent Producer in Kafka?
The Idempotent Producer in Kafka ensures that messages are delivered exactly once to a partition, even in the case of retries. It achieves this by assigning a unique ID to each produce request and maintaining a sequence number for each producer-partition pair. This prevents duplicate messages due to network issues or producer retries.

26. How does Kafka handle consumer offsets?
Kafka maintains offsets for each consumer group per partition. These offsets represent the position of the consumer in the partition log. Consumers can commit these offsets either automatically (at a configurable interval) or manually. Kafka stores these offsets in a special Kafka topic called '__consumer_offsets', allowing consumers to resume from where they left off in case of restarts or failures.

27. What is the difference between a round-robin partitioner and a key-based partitioner in Kafka?
A round-robin partitioner distributes messages evenly across all partitions in a cyclic manner, regardless of any key. A key-based partitioner, on the other hand, uses a hash of the key to determine which partition a message should go to. This ensures that all messages with the same key always go to the same partition, which is crucial for maintaining order for key-based events.

28. How does Kafka handle message deletion?
Kafka doesn't delete messages individually. Instead, it uses a retention policy to manage message deletion. Messages are retained either for a configurable amount of time or until the topic reaches a certain size. Once the retention limit is reached, Kafka deletes messages in bulk by removing whole segments of the log file. For more fine-grained control, Kafka also supports log compaction.

29. What is the purpose of the Kafka Mirror Maker?
Kafka Mirror Maker is a tool used for replicating data between Kafka clusters, potentially across different data centers. It works by consuming from one Kafka cluster and producing to another. This is useful for maintaining a backup of your data, aggregating data from multiple datacenters into a central location, or for migrating data between clusters.

30. How does Kafka handle message versioning?
Kafka itself doesn't handle message versioning directly, but it provides mechanisms that allow users to implement versioning. One common approach is to include a version field in the message schema. For more complex versioning needs, many users leverage schema registries (like the Confluent Schema Registry) which can manage schema evolution and compatibility.

31. What is the role of the controller in a Kafka cluster?
The controller in a Kafka cluster is a broker that has additional responsibilities for managing the overall state of the cluster. It's responsible for electing partition leaders, managing the distribution of partitions across brokers, and handling administrative operations like adding or removing topics. If the controller fails, ZooKeeper helps elect a new controller from among the brokers.

32. How does Kafka ensure data consistency?
Kafka ensures data consistency through several mechanisms:
Replication: Each partition is replicated across multiple brokers.
In-Sync Replicas (ISR): Only replicas that are up-to-date with the leader can be part of the ISR.
Acknowledgments: Producers can be configured to wait for acknowledgments from the leader and ISRs.
Atomic writes: Writes to a partition are atomic and ordered.
Idempotent producers: Prevent duplicate messages in case of retries.

33. What is the purpose of the Kafka AdminClient API?
The Kafka AdminClient API provides administrative operations for managing and inspecting topics, brokers, configurations, and other Kafka objects. It can be used to create, delete, and describe topics, manage ACLs, get cluster information, and perform other administrative tasks programmatically.

34. How does Kafka handle message batching?
Kafka producers can batch messages to improve throughput. Instead of sending each message individually, the producer can group multiple messages destined for the same partition into a single request. This reduces network overhead and improves efficiency. The batch size and linger time (how long to wait for more messages before sending a batch) are configurable.

35. What is the difference between a Kafka consumer and a Kafka streams application?
A Kafka consumer is a client that reads data from Kafka topics and processes it in some way. It's typically used for simple consumption scenarios. A Kafka Streams application, on the other hand, is a more sophisticated client that can consume, process, and produce data back to Kafka. It provides a DSL for complex stream processing operations like filtering, transforming, aggregating, and joining streams.

36. How does Kafka handle message ordering within a partition?
Kafka guarantees that messages within a partition are ordered. Messages sent by a producer to a specific partition will be appended to the log in the order they are sent. Consumers read messages from a partition in the exact order they were written. This ordering guarantee is crucial for use cases that require event sequencing.

37. What is the purpose of the Kafka Transactions API?
The Kafka Transactions API allows for atomic updates to multiple topics and partitions. It enables exactly-once processing semantics for applications that read, process, and write data to Kafka. This is particularly useful for stream processing applications that need to ensure that each input event affects the output exactly once, even in the face of failures.

38. How does Kafka handle message key hashing?
When a key is provided with a message, Kafka uses a hash of the key to determine which partition the message should go to. By default, Kafka uses murmur2 algorithm for key hashing. This ensures that messages with the same key always go to the same partition, which is crucial for maintaining order for key-based events and for enabling local state in stream processing applications.

39. What is the role of the Kafka consumer coordinator?
The Kafka consumer coordinator is responsible for managing the state of the consumer group and coordinating the consumer group rebalance process. It assigns partitions to consumers in the group, ensures that each partition is consumed by only one consumer in the group, and manages the committed offsets for each partition.

40. How does Kafka handle message timestamps?
Kafka supports two types of timestamps:
CreateTime: The time the producer created the message.
LogAppendTime: The time the broker received the message. These timestamps can be used for log retention, log compaction, and time-based search in consumers. The timestamp type is configurable at the topic level.
41. What is the purpose of the Kafka Quota API?
The Kafka Quota API allows you to enforce quotas on produce and fetch requests to prevent a single client from consuming too many broker resources. Quotas can be defined on a per-client or per-user basis, and can limit the rate of data production or consumption. This helps in ensuring fair resource allocation and preventing denial of service scenarios.

43. How does Kafka handle message acknowledgments?
Kafka producers can be configured to require acknowledgments when sending messages. There are three settings:
acks=0: No acknowledgment (fire and forget)
The producer doesn't wait for any acknowledgment from the broker.
This option has the lowest latency but the weakest durability guarantees since the message may be lost if the broker goes down.
acks=1: Leader acknowledgment only
The producer waits for the leader replica to acknowledge the message.
This provides better durability than acks=0, but there's still a risk of message loss if the leader fails immediately after acknowledging but before the followers have replicated the message.
acks=all: Full ISR (In-Sync Replica) acknowledgment
The producer waits for the message to be acknowledged by all in-sync replicas.
This setting provides the strongest durability guarantee but has the highest latency.
The choice of acknowledgment level affects the trade-off between latency and durability. Higher levels of acknowledgment provide stronger durability guarantees but increase latency.


44. How does Kafka handle message serialization and deserialization?
Kafka itself treats message data as opaque byte arrays and doesn't perform any serialization or deserialization. However, Kafka producers and consumers can be configured with serializers and deserializers for keys and values. Common formats include String, Integer, and Avro. For complex objects, custom serializers and deserializers can be implemented.

45. What is the purpose of the Kafka Schema Registry?
The Kafka Schema Registry provides a serving layer for metadata. It provides a RESTful interface for storing and retrieving Avro schemas. It's used in conjunction with Kafka to ensure that producers and consumers use compatible schemas. This is particularly useful in evolving data models over time while maintaining backward and forward compatibility.

46. How does Kafka handle topic deletion?
The topic is marked for deletion in ZooKeeper
Kafka stops serving data for that topic
The actual log segments on disk are asynchronously deleted This process ensures that topic deletion doesn't impact the performance of other operations. However, it's worth noting that in versions prior to Kafka 2.1, topic deletion could sometimes be incomplete if brokers were offline during the deletion process.


47. What is the difference between a Kafka consumer's poll() and subscribe() methods?
The subscribe() method is used to subscribe a consumer to one or more topics. It doesn't actually fetch any data. The poll() method, on the other hand, is used to fetch data from the subscribed topics. It returns records that have been published since the last fetch for the subscribed topics and partitions. poll() is typically called in a loop to continuously consume data.

48. How does Kafka handle message compression at the broker level?
Kafka brokers can be configured to handle message compression in several ways:
Pass-through: The broker stores the message in its original compressed format
Decompress on receipt: The broker decompresses the message on receipt and stores it uncompressed
Decompress and recompress: The broker decompresses the message and then recompresses it, potentially with a different algorithm The choice depends on factors like CPU usage, network bandwidth, and storage requirements.

49. What is the purpose of the Kafka consumer heartbeat thread?
The Kafka consumer heartbeat thread is responsible for sending periodic heartbeats to the Kafka broker (specifically, to the group coordinator). These heartbeats indicate that the consumer is alive and still part of the consumer group. If a consumer fails to send heartbeats for a configurable period, it's considered dead, and the group coordinator will trigger a rebalance to reassign its partitions to other consumers in the group.

50. How does Kafka handle message ordering across multiple partitions?
Kafka only guarantees message ordering within a single partition. Across multiple partitions, there is no guarantee of message ordering. If global ordering is required, it's typically achieved by using a single partition for the topic, but this limits scalability. For use cases requiring ordering and scalability, it's common to use a partition key that ensures related messages go to the same partition.

51. What is the role of the Kafka broker's log cleaner thread?
The log cleaner thread in Kafka is responsible for performing log compaction. Log compaction is a mechanism where Kafka removes redundant records from a log, keeping only the latest value for each key. This is useful for use cases where only the latest update for a given key is needed, such as maintaining a changelog or a database state. The log cleaner runs periodically to compact eligible topics.

52. How does Kafka handle consumer lag?
Consumer lag in Kafka refers to the difference between the offset of the last produced message and the offset of the last consumed message. Kafka provides tools and APIs to monitor consumer lag, such as the Kafka Consumer Groups command-line tool and the AdminClient API. High consumer lag can indicate performance issues or insufficient consumer capacity. Kafka doesn't automatically handle lag, but it provides the information needed for applications to make scaling or performance optimization decisions.

53. What is the purpose of the Kafka producer's Partitioner interface?
The Partitioner interface in Kafka's producer API determines which partition in the topic a message will be sent to. The default partitioner uses a hash of the key (if present) to choose the partition, ensuring that messages with the same key always go to the same partition. Custom partitioners can be implemented to control message distribution across partitions based on specific business logic or data characteristics.

54. How does Kafka handle message delivery timeouts?
Kafka producers can be configured with delivery timeouts. If a message cannot be successfully acknowledged within this timeout period, the producer will consider the send failed and may retry (depending on configuration). On the consumer side, there's a max.poll.interval.ms setting that controls how long a consumer can go without polling before it's considered failed and a rebalance is triggered.

55. What is the purpose of the Kafka Streams DSL?
The Kafka Streams DSL (Domain Specific Language) provides a high-level API for stream processing operations. It allows developers to express complex processing logic like filtering, transforming, aggregating, and joining streams of data. The DSL abstracts away many of the low-level details of stream processing, making it easier to build and maintain stream processing applications.

56. How does Kafka handle message de-duplication?
Kafka itself doesn't provide built-in de-duplication of messages. However, it provides mechanisms that allow applications to implement de-duplication:
Idempotent producers prevent duplicate messages due to producer retries.
Exactly-once semantics in Kafka Streams ensure that each input record is processed once.
For custom applications, unique message IDs can be used to detect and handle duplicates at the consumer level.

57. What is the role of the Kafka consumer's position() method?
The position() method in a Kafka consumer returns the offset of the next record that will be fetched for a given partition. This is useful for tracking the progress of consumption and can be used in conjunction with the committed() method to determine how far behind the consumer is from its last committed position. This information can be valuable for monitoring and managing consumer performance.

58. How does Kafka handle message schema evolution?
Kafka itself is agnostic to message schemas, treating messages as byte arrays. However, schema evolution is typically handled using a schema registry (like Confluent Schema Registry) in conjunction with a serialization format that supports schema evolution (like Avro). The schema registry maintains versions of schemas and ensures compatibility between producer and consumer schemas. This allows for schema changes over time while maintaining backward and forward compatibility.

59. What is the purpose of the Kafka broker's controlled shutdown?
Controlled shutdown is a feature in Kafka that allows a broker to shut down gracefully. During a controlled shutdown:
The broker stops accepting new produce requests
It completes all ongoing produce and fetch requests
It transfers leadership of its partitions to other brokers in a controlled manner This process minimizes data loss and service disruption when a broker needs to be taken offline for maintenance or other reasons.

60. How does Kafka handle message validation?
Kafka itself doesn't perform message validation beyond ensuring that messages don't exceed the configured maximum size. Message validation is typically handled at the producer or consumer level. Producers can implement validation logic before sending messages, while consumers can validate messages after receiving them. For more complex validation scenarios, intermediate processing steps (like Kafka Streams applications) can be used to validate and potentially transform messages.

61. What is the role of the Kafka consumer's commitSync() and commitAsync() methods?
These methods are used to commit offsets in Kafka consumers:
commitSync(): Synchronously commits the latest offset returned by poll(). It will retry until it succeeds or encounters a non-retriable error.
commitAsync(): Asynchronously commits offsets. It doesn't retry on failures, making it faster but less reliable than commitSync(). The choice between these methods depends on the balance between performance and reliability required by the application.

62. How does Kafka handle message retention across multiple data centers?
Kafka can handle message retention across multiple data centers through a feature called MirrorMaker. MirrorMaker is a stand-alone tool for copying data between Kafka clusters. It consumes from one cluster and produces to another, allowing for replication of data across different data centers. This can be used for disaster recovery, geographic distribution of data, or aggregating data from multiple sources into a central location.

63. What is the purpose of the Kafka producer's max.block.ms parameter?
The max.block.ms parameter in a Kafka producer controls how long the producer will block when calling send() and when explicitly requesting metadata via metadata(). If this time elapses before the producer can send the record, it will throw a TimeoutException. This parameter is useful for setting an upper bound on how long the application will wait in these scenarios, preventing indefinite blocking.

64. How does Kafka handle message consumption across consumer group rebalances?
When a consumer group rebalance occurs (due to consumers joining or leaving the group), Kafka ensures that:
All consumers stop consuming and commit their current offsets
The group coordinator reassigns partitions to the remaining consumers
Consumers start consuming from their newly assigned partitions, beginning from the last committed offset This process ensures that all messages are consumed exactly once (assuming proper offset management) even as the set of consumers changes.

65. What is the role of the Kafka broker's log.segment.bytes configuration?
The log.segment.bytes configuration in Kafka brokers controls the maximum size of a single log segment file. When a log segment reaches this size, a new segment is created. This configuration affects:
How often segments are closed and become eligible for deletion
The granularity of log retention (Kafka can only delete entire segments)
The amount of data that needs to be moved during partition reassignments Smaller segments allow for more granular retention and faster reassignments but can lead to more file handles and slightly higher overhead.

66. How does Kafka handle message consumption patterns?
Kafka supports two main consumption patterns:
Queue: Each message is processed by one consumer within a consumer group. This is achieved by having multiple consumers in a group, each reading from exclusive partitions.
Publish-Subscribe: All messages are processed by all consumers. This is achieved by having each consumer in its own consumer group, allowing all consumers to read all messages. These patterns can be combined and customized to fit various use cases.

67. What is the purpose of the Kafka producer's linger.ms parameter?
The linger.ms parameter in a Kafka producer controls the amount of time to wait for additional messages before sending a batch of messages. Increasing this value leads to larger batches and higher throughput at the cost of increased latency. Setting this to 0 (the default) means messages are sent as soon as possible. This parameter allows for fine-tuning the trade-off between latency and throughput in message production.

69. What is the role of the Kafka consumer's auto.offset.reset configuration?
The auto.offset.reset configuration in Kafka consumers determines what to do when there is no initial offset in Kafka or if the current offset no longer exists on the server. It can be set to:
earliest: automatically reset the offset to the earliest offset
latest: automatically reset the offset to the latest offset
none: throw exception to the consumer if no previous offset is found This configuration is crucial for defining behavior when a consumer starts reading from a topic for the first time or when it has been offline for a long time.

70. How does Kafka handle message retrieval for consumers?
Kafka uses a pull model for message retrieval. Consumers request messages from brokers rather than brokers pushing messages to consumers. This allows consumers to control the rate at which they receive messages. Consumers make fetch requests to brokers, specifying the topics, partitions, and starting offset for each partition. The broker responds with messages up to a specified maximum byte limit. This model allows for better flow control and makes it easier to handle scenarios where consumers fall behind.